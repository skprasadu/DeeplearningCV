{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Number of Classes: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Training Parameters\\n\",\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "# loads the MNIST dataset\\n\",\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "\n",
    "# Lets store the number of rows and columns\\n\",\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "\n",
    "# Getting our date in the right 'shape' needed for Keras\\n\",\n",
    "# We need to add a 4th dimenion to our date thereby changing our\\n\",\n",
    "# Our original image shape of (60000,28,28) to (60000,28,28,1)\\n\",\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# store the shape of a single image \\n\",\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# change our image type to float32 data type\\n\",\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\\n\",\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Now we one hot encode outputs\\n\",\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Let's count the number columns in our hot encoded matrix \\n\",\n",
    "print (\"Number of Classes: \" + str(y_test.shape[1]))\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "checkpoint = ModelCheckpoint(\"../Trained Models/MNIST_Checkpoint.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      " - 23s - loss: 0.2108 - acc: 0.9352 - val_loss: 0.0514 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05142, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 2/15\n",
      " - 22s - loss: 0.0813 - acc: 0.9760 - val_loss: 0.0417 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05142 to 0.04170, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 3/15\n",
      " - 22s - loss: 0.0638 - acc: 0.9817 - val_loss: 0.0360 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04170 to 0.03600, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 4/15\n",
      " - 23s - loss: 0.0519 - acc: 0.9846 - val_loss: 0.0324 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03600 to 0.03241, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 5/15\n",
      " - 22s - loss: 0.0459 - acc: 0.9861 - val_loss: 0.0294 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03241 to 0.02945, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 6/15\n",
      " - 22s - loss: 0.0426 - acc: 0.9875 - val_loss: 0.0368 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02945\n",
      "Epoch 7/15\n",
      " - 22s - loss: 0.0401 - acc: 0.9885 - val_loss: 0.0293 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02945 to 0.02931, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 8/15\n",
      " - 26s - loss: 0.0379 - acc: 0.9888 - val_loss: 0.0289 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02931 to 0.02889, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 9/15\n",
      " - 23s - loss: 0.0365 - acc: 0.9895 - val_loss: 0.0277 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02889 to 0.02771, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 10/15\n",
      " - 23s - loss: 0.0341 - acc: 0.9900 - val_loss: 0.0261 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02771 to 0.02614, saving model to ../Trained Models/MNIST_Checkpoint.h5\n",
      "Epoch 11/15\n",
      " - 22s - loss: 0.0332 - acc: 0.9901 - val_loss: 0.0270 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02614\n",
      "Epoch 12/15\n",
      " - 22s - loss: 0.0321 - acc: 0.9910 - val_loss: 0.0293 - val_acc: 0.9911\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02614\n",
      "Epoch 13/15\n",
      " - 28s - loss: 0.0322 - acc: 0.9905 - val_loss: 0.0291 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02614\n",
      "Epoch 14/15\n",
      " - 25s - loss: 0.0309 - acc: 0.9907 - val_loss: 0.0271 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02614\n",
      "Epoch 15/15\n",
      " - 23s - loss: 0.0311 - acc: 0.9910 - val_loss: 0.0280 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02614\n",
      "Test loss: 0.02800723246755274\n",
      "Test accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 2,\n",
    "          callbacks = callbacks,\n",
    "          validation_data = (x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\\n\",\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\\n\",\n",
    "                          patience = 3, #Number of epochs we wait before stopping \\n\",\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\\n\",\n",
    "# we put our call backs into a callback list\\n\",\n",
    "callbacks = [earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " - 28s - loss: 0.0288 - acc: 0.9915 - val_loss: 0.0313 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.02614\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=2,\n",
    "          callbacks = callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
